# EXP 5: Comparative Analysis of Different Types of Prompting Patterns

## Aim  
To test and compare how different prompting patterns — naïve/unstructured versus basic/structured — influence the quality, accuracy, and depth of ChatGPT responses across multiple scenarios, and to analyse the importance of prompt engineering in real-world AI applications.

---

## Abstract  
Prompt engineering is becoming a core skill in interacting with AI systems. This experiment investigates how two prompt types — naïve and structured — affect the output quality of ChatGPT. Four scenarios (creative story generation, factual question answering, concept summarization, and giving advice) were tested with both types of prompts. Data collected from each scenario was compared for quality, accuracy, and depth. Results show that basic prompts outperform naïve prompts by producing more detailed, accurate, and relevant responses. This study highlights the necessity of prompt refinement to achieve optimal AI performance and serves as a guide for students and professionals seeking to maximize the benefits of AI tools.

---

## Introduction  
Large Language Models (LLMs) such as ChatGPT rely heavily on input prompts to generate meaningful outputs. A “prompt” is the instruction or question we give the model. The better and more structured the prompt, the higher the chance of obtaining a useful response. Prompt engineering thus emerges as an essential skill for professionals, educators, and researchers.  

This experiment demonstrates how changing the wording, detail, and constraints in a prompt can affect ChatGPT’s responses. We compare two styles:

- **Naïve prompts**: simple, vague, open-ended.
- **Basic prompts**: structured, detailed, context-rich, and specific.  

By running multiple scenarios, we show that prompt quality directly influences output quality.

---

## Theory / Background  

### Large Language Models  
LLMs such as GPT-4 and GPT-5 are trained on massive datasets to predict the next word in a sequence. They can write essays, summarize texts, generate stories, and more. However, their performance depends strongly on how the input is phrased.  

### Prompt Engineering  
Prompt engineering is the practice of designing and refining prompts to obtain the desired response. Well-crafted prompts act as a roadmap, guiding the model to stay on topic, include or exclude certain details, and format its output.

### Types of Prompts  
- **Naïve Prompts**: Broad, vague, or too simple instructions.  
- **Basic Prompts**: Clear, detailed, structured instructions with context and constraints.  

---

## Test Scenarios and Prompt Design  

| Scenario | Naïve Prompt | Basic Prompt |
|----------|--------------|--------------|
| Creative Story | “Write a story about a dragon.” | “Write a 300-word story about a dragon who befriends a lost child in a magical forest. Include setting description, conflict, and resolution.” |
| Factual Question | “What is climate change?” | “Explain climate change in detail, including causes, effects on humans and nature, and 3 possible solutions. Use clear, structured paragraphs.” |
| Summarizing Concept | “Summarize photosynthesis.” | “Summarize the process of photosynthesis in less than 150 words. Cover inputs, outputs, and importance for plants and humans.” |
| Advice / Recommendations | “How can I stay healthy?” | “Provide 5 practical tips for maintaining good health as a college student with limited time and budget. Organize them as bullet points.” |

---

## Experimental Procedure  

1. Select four scenarios covering creative, factual, summarizing, and advisory tasks.  
2. Prepare two prompt types per scenario.  
3. Run each prompt through ChatGPT and record the outputs in a document.  
4. Evaluate responses on three criteria: **Quality**, **Accuracy**, **Depth**.  
5. Organize data in a comparative table.  
6. Create charts/graphs for visual analysis.  
7. Write conclusions and observations.

---

## Results  

| Scenario | Naïve Prompt Response (Summary) | Basic Prompt Response (Summary) | Comparison (Quality, Accuracy, Depth) |
|----------|---------------------------------|---------------------------------|---------------------------------------|
| Story | Generic plot, lacks detail or structure. | Clear 300-word story, richer vocabulary, setting, conflict, resolution. | Basic prompt gives superior quality and depth. |
| Factual Question | Only definition of climate change. | Causes, effects, and solutions in structured format. | Basic prompt improved accuracy and organization. |
| Summary | Long, sometimes incomplete. | Concise 150-word summary with inputs/outputs/importance. | Basic prompt enforced clarity and brevity. |
| Advice | Generic tips like “eat healthy.” | Tailored, practical advice for students (sleep, budgeting, exercise). | Basic prompt gave more practical and relevant tips. |

---

## Graphical Representation  

### Quality, Accuracy, Depth Comparison  
<img width="2379" height="1180" alt="image" src="https://github.com/user-attachments/assets/a1a7e99d-ca9b-4646-be58-67f0bed4a6b6" />
 

Example chart:  
- Naïve Prompt: lower scores (Quality 4–5/10).  
- Basic Prompt: higher scores (Quality 8–9/10).  

---

## Analysis  

- **Clarity matters:** Structured prompts produced more accurate, detailed, and user-relevant results.  
- **Naïve prompts performed decently** only for very general topics but lacked structure for complex tasks.  
- **Basic prompts guided ChatGPT** to organize information, respect word limits, and address specific needs.  
- **Depth and Quality increased significantly** with structured prompts, especially in storytelling and recommendations.  
- **Efficiency improved:** Less need for follow-up prompts to clarify.  

---

## Applications of Prompt Engineering  

- **Education:** Creating custom lesson plans, summarizing complex topics for students.  
- **Healthcare:** Generating patient-friendly explanations of medical conditions.  
- **Content Creation:** Producing blogs, marketing copy, or scripts.  
- **Business Intelligence:** Summarizing market reports or datasets.  
- **Research Assistance:** Drafting research outlines, abstracts, and literature reviews.  

---

## Discussion of Findings  

The comparative analysis shows that the way a question is phrased directly influences the usefulness of ChatGPT’s output. Even small changes, such as adding context or specifying output format, drastically improve quality. Naïve prompts often led to generic or incomplete answers, while structured prompts gave more actionable, targeted content.

---

## Conclusion  

This experiment successfully demonstrated the impact of prompt structure on AI-generated responses. Basic prompts consistently produced better results across all four scenarios, confirming that prompt engineering is not optional but essential for professional, academic, and creative tasks. Future work may explore advanced prompting patterns such as role-based prompting, chain-of-thought prompting, and multi-turn refinement.

---

 

